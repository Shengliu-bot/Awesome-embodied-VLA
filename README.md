# Awesome-embodied-VLA

This repo contains a curative list of **papers using VLM/VLA**.

* **VoxPoser**: "Voxposer: Composable 3D Value Maps for Robotic Manipulation with Language Models", *arXiv, Nov 2023*. [[Paper](https://voxposer.github.io/voxposer.pdf)] [[Code](https://github.com/huangwl18/VoxPoser)] [[Website](https://voxposer.github.io/)]

* **ReKep**: "ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation", *arXiv, Sep 2024*. [[Paper](https://rekep-robot.github.io/rekep.pdf)] [[Code](https://github.com/huangwl18/ReKep)] [[Website](https://rekep-robot.github.io)]

* **RoboNurse-VLA**: "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action", *arXiv, Sep 2024*. [[Paper](https://arxiv.org/abs/2409.19590)][[Website](https://arxiv.org/abs/2409.19590)]

* **TinyVLA**: "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation", *arXiv, Sep 2024*. [[Paper](https://arxiv.org/pdf/2409.12514)]

* **EdgeVLA**: "EdgeVLA: Efficient Vision-Language-Action Models", *Nov 2024*. [[Paper](https://kscale-public.s3.amazonaws.com/evla_09092024/report.pdf)]
